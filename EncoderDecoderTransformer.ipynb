{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "ak4XDwqWnsS7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n",
        "# For new language translation will need to download vocabulary.\n",
        "# Note that I have used two different sources.\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "QH39FVFnH-x0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dcc2956-0696-4b6f-96c5-6645df485d99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "de-core-news-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.2.0 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.2.0 which is incompatible.\n",
            "inflect 6.0.5 requires pydantic<2,>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m2023-08-07 14:07:50.077651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-07 14:07:51.392980: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl#egg=de_core_news_sm==3.2.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting de-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.1.3)\n",
            "Installing collected packages: de-core-news-sm\n",
            "  Attempting uninstall: de-core-news-sm\n",
            "    Found existing installation: de-core-news-sm 3.5.0\n",
            "    Uninstalling de-core-news-sm-3.5.0:\n",
            "      Successfully uninstalled de-core-news-sm-3.5.0\n",
            "Successfully installed de-core-news-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "2023-08-07 14:08:03.857428: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-07 14:08:05.174768: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl#egg=en_core_web_sm==3.2.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.5.0\n",
            "    Uninstalling en-core-web-sm-3.5.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.5.0\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qfuQxY7ICbP",
        "outputId": "b403fd33-3159-410f-8141-71f29ef088ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from torch.nn.functional import pad\n",
        "import os\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch"
      ],
      "metadata": {
        "id": "R-ZZwUJXKU28"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"drive/MyDrive/Colab Notebooks/datasets\")\n",
        "os.getcwd()\n",
        "os.listdir()\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "VCq4DqOeICgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c910bbfb-1e59-400c-e9d8-d6f338273d3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DW2rnldMk3Np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273f637a-0fcf-4de8-b286-6a348ce07829"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78c2cd99cc70>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "num_heads = 4\n",
        "dropout = 0.0\n",
        "n_embd = 64\n",
        "lr_rate = 1e-3\n",
        "n_head = 4\n",
        "device = 'cpu' if torch.cuda.is_available else 'cpu'\n",
        "max_iters = 1000\n",
        "num_batch =  4\n",
        "eval_interval = 100\n",
        "eval_iters = 200\n",
        "n_layer = 4\n",
        "num_layers = 4\n",
        "start_token = torch.tensor([0], dtype=torch.int64)\n",
        "end_token = torch.tensor([1], dtype = torch.int64)\n",
        "pad_token = torch.tensor([2], dtype = torch.int64)\n",
        "max_length = 128\n",
        "block_size = max_length\n",
        "#----\n",
        "torch.manual_seed(1337)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSvQ5_BiI__D",
        "outputId": "4972ba95-f7df-4494-ee65-a3175a8371d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "europarl-v7.de-en.de  multi30k_model_00.pt  multi30k_model_03.pt     vocab.pt\n",
            "europarl-v7.de-en.en  multi30k_model_01.pt  multi30k_model_final.pt\n",
            "HarvardTransformer    multi30k_model_02.pt  Untitled0.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_file_en = \"europarl-v7.de-en.en\"\n",
        "my_file_de = \"europarl-v7.de-en.de\"\n",
        "\n",
        "#open these files and st\n",
        "size_train, size_val, size_test = 10000, 4000, 4000\n",
        "train, val, test = [], [], []\n",
        "de_list, en_list = [], []\n",
        "\n",
        "count = 0"
      ],
      "metadata": {
        "id": "p0KWANiJLH-1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#opened a file and stored each line in a list\n",
        "\n",
        "with open(my_file_de, encoding=\"utf8\") as fp:\n",
        "  for line in fp:\n",
        "    de_list.append(line)\n",
        "    if count > size_train + size_val + size_test - 1:\n",
        "      break\n",
        "count = 0"
      ],
      "metadata": {
        "id": "D6J0zTr_LJRy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(my_file_en, encoding=\"utf8\") as fp:\n",
        "  for line in fp:\n",
        "    en_list.append(line)\n",
        "    if count > size_train + size_val + size_test - 1:\n",
        "      break\n",
        "count = 0"
      ],
      "metadata": {
        "id": "m7PbJHegLLhH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = list(zip(de_list,en_list))\n",
        "train, val, test = dataset[:size_train], dataset[size_train:size_train+size_val], dataset[size_train+size_val:size_train+size_val+size_test]"
      ],
      "metadata": {
        "id": "fWsXfI7rLN4r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#each language has a tokenizer,made a fn to stored em up\n",
        "\n",
        "def load_tokenizers():\n",
        "\n",
        "    try:\n",
        "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "    except IOError:\n",
        "        os.system(\"python -m spacy download de_core_news_sm\")\n",
        "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "\n",
        "    try:\n",
        "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "    except IOError:\n",
        "        os.system(\"python -m spacy download en_core_web_sm\")\n",
        "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    return spacy_de, spacy_en"
      ],
      "metadata": {
        "id": "Yt2itei-LRi7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_de, spacy_en = load_tokenizers()"
      ],
      "metadata": {
        "id": "lgnpD7tHMTMe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, tokenizer):\n",
        "  return [tok.text for tok in tokenizer.tokenizer(text)]"
      ],
      "metadata": {
        "id": "0waslZEULTHA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"I'm become death the destroyer of worls\", spacy_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYgC0bc3NN1y",
        "outputId": "5e3bd98b-4984-475e-bad8-cca7ca85da8a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', \"'m\", 'become', 'death', 'the', 'destroyer', 'of', 'worls']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter,tokenizer,index):\n",
        "  for from_to_tuples in data_iter:\n",
        "    yield tokenizer(from_to_tuples[index])"
      ],
      "metadata": {
        "id": "diN314FbLU8H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(spacy_de, spacy_en, train, val, test):\n",
        "  def tokenize_en(text):\n",
        "    return tokenize(text, spacy_en)\n",
        "  def tokenize_de(text):\n",
        "    return tokenize(text, spacy_de)\n",
        "\n",
        "  vocab_src = build_vocab_from_iterator(yield_tokens(train+val+test, spacy_de, 0),\n",
        "                                        min_freq = 2,\n",
        "                                        specials = [\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"])\n",
        "\n",
        "  vocab_tar = build_vocab_from_iterator(yield_tokens(train+val+test, spacy_en, 1),\n",
        "                                        min_freq = 2,\n",
        "                                        specials = [\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"])\n",
        "\n",
        "  vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
        "  vocab_src.set_default_index(vocab_tar[\"<unk>\"])\n",
        "\n",
        "  return vocab_src, vocab_tar"
      ],
      "metadata": {
        "id": "qv5GZm4MLWB2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_src, vocab_tar = build_vocabulary(spacy_de, spacy_en, train, val, test)\n",
        "# vocab_src, vocab_tar"
      ],
      "metadata": {
        "id": "YgiEr2ZzLX00"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vocab(spacy_de, spacy_dn):\n",
        "  if not os.path.exists(\"vocab.pt\"):\n",
        "      vocab_src, vocab_tar = build_vocabulary(spacy_de, spacy_en, train, val, test)\n",
        "      torch.save((vocab_src,vocab_tar),\"vocab.pt\")\n",
        "  else:\n",
        "    vocab_src, vocab_tar = torch.load(\"vocab.pt\")\n",
        "  print(len(vocab_src), len(vocab_tar))\n",
        "  return vocab_src, vocab_tar"
      ],
      "metadata": {
        "id": "a8y29FJfI-jr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_src, vocab_tar = torch.load(\"vocab.pt\")"
      ],
      "metadata": {
        "id": "RO8b4JsdNwRR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size_src, vocab_size_tgt = (len(vocab_src), len(vocab_tar))"
      ],
      "metadata": {
        "id": "eGmQ18gSQIjT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(\n",
        "    batch,\n",
        "    device,\n",
        "    max_length = max_length,\n",
        "    pad_id = 2\n",
        "):\n",
        "  start_token = torch.tensor([0], dtype=torch.int64)\n",
        "  end_token = torch.tensor([1], dtype = torch.int64)\n",
        "\n",
        "  src_list = []\n",
        "  tar_list = []\n",
        "\n",
        "  for (txt_src, txt_tar) in batch:\n",
        "    processed_text = torch.cat(\n",
        "        [start_token,\n",
        "        torch.tensor(\n",
        "            vocab_src(tokenize(txt_src, spacy_de)),\n",
        "            dtype = torch.int64,\n",
        "            device=device\n",
        "            ),\n",
        "        end_token],\n",
        "        0\n",
        "    )\n",
        "\n",
        "    final_text = torch.cat([\n",
        "        start_token,\n",
        "        torch.tensor(\n",
        "            vocab_tar(tokenize(txt_tar, spacy_en)),\n",
        "            dtype = torch.int64,\n",
        "            device=device),\n",
        "        end_token],0\n",
        "    )\n",
        "    src_list.append(\n",
        "        pad(\n",
        "            processed_text,\n",
        "            (0, max_length - len(processed_text)),\n",
        "            value = pad_id\n",
        "        )\n",
        "    )\n",
        "    tar_list.append(\n",
        "        pad(\n",
        "          final_text,\n",
        "          (0,max_length - len(final_text)),\n",
        "          value = pad_id\n",
        "        )\n",
        "    )\n",
        "  eng_tensor = torch.stack(src_list)\n",
        "  ger_tensor = torch.stack(tar_list)\n",
        "\n",
        "  return eng_tensor, ger_tensor"
      ],
      "metadata": {
        "id": "XDahbfgMKzWk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def src_convertor(strings, device=device, max_length=128, pad_id=2):\n",
        "    start_token = torch.tensor([0], dtype=torch.int64)\n",
        "    end_token = torch.tensor([1], dtype=torch.int64)\n",
        "    src_tensor_list = []\n",
        "\n",
        "    for string in strings:\n",
        "        processed_text = torch.cat(\n",
        "            [start_token,\n",
        "            torch.tensor(\n",
        "                vocab_src(tokenize(string, spacy_de)),\n",
        "                dtype=torch.int64,\n",
        "                device=device\n",
        "                ),\n",
        "            end_token],\n",
        "            0\n",
        "        )\n",
        "\n",
        "        src_tensor = torch.tensor(\n",
        "            pad(\n",
        "                processed_text,\n",
        "                (0, max_length - len(processed_text)),\n",
        "                value=pad_id\n",
        "            )\n",
        "        )\n",
        "\n",
        "        src_tensor_list.append(src_tensor)\n",
        "\n",
        "    return torch.stack(src_tensor_list)\n",
        "\n",
        "# Test with a list of two strings\n",
        "strings = [\"Hello world\", \"This is a test\"]\n",
        "src_convertor(strings)"
      ],
      "metadata": {
        "id": "SktY11AyY5RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f54c98-0f39-428c-fe43-e471a4fe8b78"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-8d9b37bd1c9e>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  src_tensor = torch.tensor(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0,     3,     3,     1,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2],\n",
              "        [    0,     3, 12305,  4978,     3,     1,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tar_convertor(\n",
        "    string,\n",
        "    device = device,\n",
        "    max_length =128,\n",
        "    pad_id = 2\n",
        "):\n",
        "  start_token = torch.tensor([0], dtype=torch.int64)\n",
        "  end_token = torch.tensor([1], dtype = torch.int64)\n",
        "  tar_list = []\n",
        "  final_text = torch.cat([\n",
        "        start_token,\n",
        "        torch.tensor(\n",
        "            vocab_tar(tokenize(string, spacy_en)),\n",
        "            dtype = torch.int64,\n",
        "            device=device),\n",
        "        end_token],0\n",
        "    )\n",
        "\n",
        "  tar_tensor = torch.tensor(\n",
        "        pad(\n",
        "          final_text,\n",
        "          (0,max_length - len(final_text)),\n",
        "          value = pad_id\n",
        "        )\n",
        "    )\n",
        "\n",
        "  return tar_tensor"
      ],
      "metadata": {
        "id": "pKKVQKiQZgM5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkl = tar_convertor(\"i am death\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xVBf8U2auWW",
        "outputId": "23e69881-5325-425b-d78e-33f906bfe4e8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-e5e8b25c9381>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tar_tensor = torch.tensor(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y  = collate_batch(train, \"cpu\")\n",
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZdeYye_K4dt",
        "outputId": "a20f80af-0414-4e1f-c2b5-a550eb68e520"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10000, 128]), torch.Size([10000, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCShODXc0Jro",
        "outputId": "adb81e65-0667-4881-e94f-5334184c5723"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   0, 3985,    8, 2424,    5,    1,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "           2,    2,    2,    2,    2,    2,    2,    2])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batch(split):\n",
        "  input_data, output_data = collate_batch(val, \"cpu\") if split == \"val\" else collate_batch(train, \"cpu\")\n",
        "  return input_data, output_data"
      ],
      "metadata": {
        "id": "6MI6ZZqtOPGW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = prepare_batch(\"train\")\n",
        "X_val, Y_val = prepare_batch(\"val\")"
      ],
      "metadata": {
        "id": "5w_5ckniOQeN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  input_data, output_data = (X_val, Y_val) if split == \"val\" else (X_train, Y_train)\n",
        "  idx = torch.randint(input_data.shape[0],(batch_size,))\n",
        "  xb = torch.stack([input_data[ix] for ix in idx])\n",
        "  yb = torch.stack([output_data[ix] for ix in idx])\n",
        "\n",
        "  return xb, yb"
      ],
      "metadata": {
        "id": "4g8sy38NOSRa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sequences(tensor):\n",
        "  # Replace end token with pad token\n",
        "  tensor1 = tensor.clone()\n",
        "  tensor1[tensor1 == end_token] = pad_token\n",
        "\n",
        "  # Remove start token and pad at the end\n",
        "  tensor2 = tensor[:, 1:]  # Remove start token\n",
        "  # Append a pad token at the end\n",
        "  tensor2 = torch.cat([tensor2, torch.full((tensor2.shape[0], 1), 2,dtype=tensor.dtype, device=tensor.device)], dim=1)\n",
        "\n",
        "  return tensor1, tensor2"
      ],
      "metadata": {
        "id": "mFGup3jA0hMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output = m.generate(context, max_new_tokens=50)[0].tolist()\n",
        "def output_convertor(tensors):\n",
        "    model_txts = []\n",
        "    for tense in tensors:\n",
        "        model_txt = (\n",
        "            \" \".join(\n",
        "                [vocab_tar.get_itos()[x] for x in tense if x != 2]\n",
        "            ).split(\"</s>\", 1)[0]\n",
        "            + \"</s>\"\n",
        "        )\n",
        "        model_txt = model_txt.replace(\"\\n\", \"\")\n",
        "        model_txts.append(model_txt)\n",
        "    return model_txts\n",
        "#print(\"Model Output               : \" + model_txt.replace(\"\\n\", \"\"))"
      ],
      "metadata": {
        "id": "gzUhsgQ6b8r_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xb, yb = get_batch(\"train\")\n",
        "# xb.shape, yb.shape"
      ],
      "metadata": {
        "id": "ChcERQSivqyA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Head,LayerNorm,MultiHead,FeedForward, Block\n",
        "class Head(nn.Module):\n",
        "  def __init__(self,num_heads):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, num_heads, bias = False)\n",
        "    self.query = nn.Linear(n_embd, num_heads, bias = False)\n",
        "    self.value = nn.Linear(n_embd, num_heads, bias = False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, Q,K,V, mask_para=True):\n",
        "    B,T,C = Q.shape\n",
        "    k = self.key(K)\n",
        "    q = self.query(Q)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    if mask_para == True:\n",
        "      wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "    wei = self.dropout(wei)\n",
        "    v = self.value(V)\n",
        "    out = wei @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "pFa9HYDLuP8U"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(4,8,64)\n",
        "h = Head(4)\n",
        "out = h(x,x,x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoOfqmUmEEey",
        "outputId": "fe4e43dd-b77e-4871-c429-399497fd1219"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.layer = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, Q,K,V, mask_para=False):\n",
        "    output = torch.cat([head(Q,K,V, mask_para) for head in self.heads], dim=-1)\n",
        "    output = self.dropout(self.layer(output))\n",
        "    return output"
      ],
      "metadata": {
        "id": "OPidKOpWAU2o"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mult = MultiHeadAttention(4,16)\n",
        "x = torch.randn(4,8,64)\n",
        "out = mult(x,x,x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc6vTlrpCWGB",
        "outputId": "a57080ac-9313-49de-f95f-1d55b0ad4c26"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.sequence = nn.Sequential(\n",
        "      nn.Linear(n_embd, 4*n_embd),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4*n_embd, n_embd),\n",
        "      nn.Dropout(dropout)\n",
        "  )\n",
        "  def forward(self,x):\n",
        "    return self.sequence(x)"
      ],
      "metadata": {
        "id": "0pXrSVpfGLF3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, n_embd, num_heads):\n",
        "    super().__init__()\n",
        "    self.self_attn = MultiHeadAttention(num_heads, n_embd//num_heads)\n",
        "    self.feed_fwd = FeedForward(n_embd)\n",
        "    self.layer1 = nn.LayerNorm(n_embd)\n",
        "    self.layer2 = nn.LayerNorm(n_embd)\n",
        "  def forward(self, x):\n",
        "    layered_input = self.layer1(x)\n",
        "    x = x + self.self_attn(layered_input,layered_input, layered_input, mask_para=False)\n",
        "    x = x + self.feed_fwd(self.layer2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "tRjJ5nDI2y5g"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define 2 attention layers and 1 Ffwd layer\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, n_embd, num_heads):\n",
        "    super().__init__()\n",
        "    self.self_attn = MultiHeadAttention(num_heads, n_embd//num_heads)\n",
        "    self.cross_attn = MultiHeadAttention(num_heads, n_embd//num_heads)\n",
        "    self.Ffw = FeedForward(n_embd)\n",
        "    self.layer1 = nn.LayerNorm(n_embd)\n",
        "    self.layer2 = nn.LayerNorm(n_embd)\n",
        "    self.layer3 = nn.LayerNorm(n_embd)\n",
        "  def forward(self, x, enc_output, src_mask=False, tgt_mask=True):\n",
        "    x = x + self.self_attn(self.layer1(x), self.layer1(x), self.layer1(x), mask_para=tgt_mask) #Decoder-only layer\n",
        "    x = x + self.cross_attn(self.layer2(x), self.layer2(enc_output), self.layer2(enc_output), mask_para=src_mask) #Encoder-decoder\n",
        "    x = x + self.Ffw(self.layer3(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "CRRk6vdh55_s"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(4,8,64)\n",
        "enc = EncoderBlock(64,4)\n",
        "enc_X = enc(x)\n",
        "enc_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctc1L0gQCRxe",
        "outputId": "0aa87fc8-beb1-443f-e468-4c4862037553"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dec = DecoderBlock(64,4)\n",
        "dec_X = dec(x, enc_X, False,True)\n",
        "dec_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejCgmlDhE6Mi",
        "outputId": "06791b90-0227-443b-be27-7430434c8259"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = BigramLanguageModel()\n",
        "# m = model.to(device)\n",
        "# logits, loss = m(xb, yb)\n",
        "# logits.shape, loss.shape\n",
        "# loss.item()"
      ],
      "metadata": {
        "id": "GSijiDMPT2ab"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab_size_src, vocab_size_tgt = (vocab_size, vocab_size)\n",
        "context = torch.zeros((1,1), dtype = torch.long, device=device)"
      ],
      "metadata": {
        "id": "mp5dHmOWHbp0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch(\"train\")\n",
        "block_size = xb.shape[1]"
      ],
      "metadata": {
        "id": "KsPcLneZThp8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So hear this out:- treat the src tensor like a constant, divide the target tensor into 2 parts- one goes from 0 to n-2, the other goes from 1 to n-1.\n",
        "#How will generation happen? So the basic issue is that pos embedding, used the T of the input tensor. Now I am using the src tensor for pos embedding, hence leading to issues\n",
        "#just make another position embd and you will be fine. Initialize another, don't define another"
      ],
      "metadata": {
        "id": "Pl0ouK1BgVtO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the game\n",
        "#Module List of encoder & decoder needs to be made\n",
        "#initially pass through a src & tgt. src will be passed thru encoder, and tgt directly to decoder\n",
        "#t & T need to be same.\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_src = nn.Embedding(vocab_size_src,n_embd) #B,T\n",
        "    self.token_embedding_tgt = nn.Embedding(vocab_size_tgt,n_embd) #B,T\n",
        "    self.position_embedding = nn.Embedding(max_length, n_embd) #T,C\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([EncoderBlock(n_embd, num_heads) for _ in range(num_layers)])\n",
        "    self.decoder_layers = nn.ModuleList([DecoderBlock(n_embd, num_heads) for _ in range(num_layers)])\n",
        "\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.layer = nn.Linear(n_embd, vocab_size_tgt)\n",
        "\n",
        "  def split_tensor(tensor):\n",
        "    return tensor[:, :-1], tensor[:, 1:]\n",
        "\n",
        "  def process_sequences(tensor):\n",
        "    # Replace end token with pad token\n",
        "    tensor1 = tensor.clone()\n",
        "    tensor1[tensor1 == end_token] = pad_token\n",
        "\n",
        "    # Remove start token and pad at the end\n",
        "    tensor2 = tensor[:, 1:]  # Remove start token\n",
        "    # Append a pad token at the end\n",
        "    tensor2 = torch.cat([tensor2, torch.full((tensor2.shape[0], 1), 2, dtype=tensor.dtype, device=tensor.device)], dim=1)\n",
        "\n",
        "    return tensor1, tensor2\n",
        "\n",
        "  def forward(self, src,tgt=context,state=\"train\"):\n",
        "    B, T = src.shape\n",
        "    #print(f\"src shape tensors {B} {T}\")\n",
        "    #just divide tgt into 2 tensors, _inp & _out\n",
        "    #b,t = tgt.shape\n",
        "    tgt_inp, tgt_out = process_sequences(tgt) #B, T\n",
        "    #print(f\"src shapes {B} {T}\")\n",
        "    tok_src = self.token_embedding_src(src) #B,T,C\n",
        "    pos = self.position_embedding(torch.arange(T, device = src.device)) #T, C\n",
        "\n",
        "    tok_tar = self.token_embedding_tgt(tgt_inp) #B,t,C\n",
        "    #pos_tar = self.position_embedding(torch.arange(t-1, device = src.device)) #t,C\n",
        "\n",
        "    #print(f\"tgt shapes {tgt.shape}; {tgt_inp.shape}, {tgt_out.shape}\")\n",
        "    enc_X = tok_src + pos #B,T,C + T,C (broad)\n",
        "    #print(f\"shapes {tok_tar.shape}, {pos.shape}\")\n",
        "    #print(pos)\n",
        "    dec_X = tok_tar + pos #B,t,C + t,C\n",
        "\n",
        "    for enc_layer in self.encoder_layers:\n",
        "      enc_output = enc_layer(enc_X)\n",
        "\n",
        "    for dec_layer in self.decoder_layers:\n",
        "      dec_output = dec_layer(dec_X, enc_X) #There will be an issue here!\n",
        "\n",
        "    x = self.ln_f(dec_output)\n",
        "    logits = self.layer(x)\n",
        "\n",
        "    if state != \"train\":\n",
        "      return logits\n",
        "    else:\n",
        "      B, T, vocab = logits.shape\n",
        "      logits = logits.view(B*T, vocab)\n",
        "      targets = tgt_out.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      return logits, loss\n",
        "\n",
        "  def generate(self, src_idx):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    b, t = src_idx.shape\n",
        "    tgx = torch.full((b, t), 2, dtype=torch.long) #B', T So the input will be padded once sent, so each will have max_length T.\n",
        "    tgx[:, 0] = start_token\n",
        "    tgx[:, 1] = end_token\n",
        "    for i in range(max_length-2):\n",
        "        #tgx = tgx[:, -block_size:] #is it required? No\n",
        "        logits = self(src_idx, tgx,state=\"generate\") #passing B',T and B'T ; B',T, vocab\n",
        "        logits = logits[:, i, :] # becomes (B, vocab) ; only taking the outputs for the last token (this is wrong), u need to find the end_token and take that.Changed it\n",
        "        probs = F.softmax(logits, dim=-1) # (B, vocab)\n",
        "        tgx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "        tgx[:,i+1] = tgx_next #just change the next token after the one whose pred we considered\n",
        "        if tgx_next == end_token:\n",
        "          break\n",
        "    return tgx"
      ],
      "metadata": {
        "id": "FktJAv7XMSOH"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First add end tokens to tgx 1\n",
        "#Now take the logits of the element whose generation we are at 1\n",
        "#The tgx_next should be then added after the generation index element while not increasing the total length 1\n",
        "#The for loop doesn't make sense. End the loop when end_token comes or when max_sequence_length is iterated 1"
      ],
      "metadata": {
        "id": "P7wrJWcrAb_x"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer()\n",
        "\n",
        "\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "QIB6bCvWJvKQ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write the training loop for it\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=lr_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  #get new xb, yb\n",
        "  xb, yb = get_batch(\"train\")\n",
        "  #do fwd pass\n",
        "  logits, loss = m(xb, yb)\n",
        "  if iter % 10 == 0:\n",
        "    print(f\"Step no. {iter}. Loss is {loss}\")\n",
        "  #make grads 0\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  #do backprop over em\n",
        "  loss.backward()\n",
        "  #update the paras\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "Gmat5AaIDqtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a1fad9-8c4d-4c8d-88d5-0f6464d2b7ee"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step no. 0. Loss is 9.568796157836914\n",
            "Step no. 10. Loss is 6.658602237701416\n",
            "Step no. 20. Loss is 5.739870071411133\n",
            "Step no. 30. Loss is 4.091020584106445\n",
            "Step no. 40. Loss is 3.000520706176758\n",
            "Step no. 50. Loss is 2.8292644023895264\n",
            "Step no. 60. Loss is 2.1066696643829346\n",
            "Step no. 70. Loss is 1.9965558052062988\n",
            "Step no. 80. Loss is 1.6645666360855103\n",
            "Step no. 90. Loss is 1.732751727104187\n",
            "Step no. 100. Loss is 1.5227794647216797\n",
            "Step no. 110. Loss is 1.5882574319839478\n",
            "Step no. 120. Loss is 1.7026495933532715\n",
            "Step no. 130. Loss is 1.523154854774475\n",
            "Step no. 140. Loss is 1.3931849002838135\n",
            "Step no. 150. Loss is 1.7651056051254272\n",
            "Step no. 160. Loss is 1.2777141332626343\n",
            "Step no. 170. Loss is 1.52734375\n",
            "Step no. 180. Loss is 1.3272706270217896\n",
            "Step no. 190. Loss is 1.422908067703247\n",
            "Step no. 200. Loss is 1.4817280769348145\n",
            "Step no. 210. Loss is 1.605871558189392\n",
            "Step no. 220. Loss is 1.269909143447876\n",
            "Step no. 230. Loss is 1.5243968963623047\n",
            "Step no. 240. Loss is 1.7381142377853394\n",
            "Step no. 250. Loss is 1.60524582862854\n",
            "Step no. 260. Loss is 1.0801451206207275\n",
            "Step no. 270. Loss is 1.570756435394287\n",
            "Step no. 280. Loss is 1.568848729133606\n",
            "Step no. 290. Loss is 1.1461464166641235\n",
            "Step no. 300. Loss is 1.2207318544387817\n",
            "Step no. 310. Loss is 1.7296069860458374\n",
            "Step no. 320. Loss is 1.4814337491989136\n",
            "Step no. 330. Loss is 1.4624536037445068\n",
            "Step no. 340. Loss is 1.5181101560592651\n",
            "Step no. 350. Loss is 1.1256849765777588\n",
            "Step no. 360. Loss is 1.4034470319747925\n",
            "Step no. 370. Loss is 1.4929933547973633\n",
            "Step no. 380. Loss is 1.3074957132339478\n",
            "Step no. 390. Loss is 1.5188624858856201\n",
            "Step no. 400. Loss is 1.0843791961669922\n",
            "Step no. 410. Loss is 1.3488191366195679\n",
            "Step no. 420. Loss is 1.41795814037323\n",
            "Step no. 430. Loss is 1.4294085502624512\n",
            "Step no. 440. Loss is 1.296378493309021\n",
            "Step no. 450. Loss is 1.0879079103469849\n",
            "Step no. 460. Loss is 1.3920423984527588\n",
            "Step no. 470. Loss is 1.51614511013031\n",
            "Step no. 480. Loss is 1.6268091201782227\n",
            "Step no. 490. Loss is 1.387994647026062\n",
            "Step no. 500. Loss is 1.481925368309021\n",
            "Step no. 510. Loss is 1.3727664947509766\n",
            "Step no. 520. Loss is 1.6820335388183594\n",
            "Step no. 530. Loss is 1.1422189474105835\n",
            "Step no. 540. Loss is 1.2791166305541992\n",
            "Step no. 550. Loss is 1.1917554140090942\n",
            "Step no. 560. Loss is 1.3969801664352417\n",
            "Step no. 570. Loss is 0.9051310420036316\n",
            "Step no. 580. Loss is 1.3973170518875122\n",
            "Step no. 590. Loss is 1.0933153629302979\n",
            "Step no. 600. Loss is 0.9686594009399414\n",
            "Step no. 610. Loss is 0.9754067063331604\n",
            "Step no. 620. Loss is 1.0150058269500732\n",
            "Step no. 630. Loss is 1.3476110696792603\n",
            "Step no. 640. Loss is 1.2495700120925903\n",
            "Step no. 650. Loss is 1.2371488809585571\n",
            "Step no. 660. Loss is 1.309831976890564\n",
            "Step no. 670. Loss is 1.1924046277999878\n",
            "Step no. 680. Loss is 0.9705268144607544\n",
            "Step no. 690. Loss is 1.1056272983551025\n",
            "Step no. 700. Loss is 1.2497535943984985\n",
            "Step no. 710. Loss is 1.2844908237457275\n",
            "Step no. 720. Loss is 0.9002501964569092\n",
            "Step no. 730. Loss is 1.4306203126907349\n",
            "Step no. 740. Loss is 1.1342573165893555\n",
            "Step no. 750. Loss is 1.3319447040557861\n",
            "Step no. 760. Loss is 1.164316177368164\n",
            "Step no. 770. Loss is 1.0491161346435547\n",
            "Step no. 780. Loss is 1.1757709980010986\n",
            "Step no. 790. Loss is 0.9890803098678589\n",
            "Step no. 800. Loss is 1.4960730075836182\n",
            "Step no. 810. Loss is 1.0241115093231201\n",
            "Step no. 820. Loss is 1.342227816581726\n",
            "Step no. 830. Loss is 1.362696886062622\n",
            "Step no. 840. Loss is 1.2414329051971436\n",
            "Step no. 850. Loss is 1.4306366443634033\n",
            "Step no. 860. Loss is 1.1695570945739746\n",
            "Step no. 870. Loss is 1.149110198020935\n",
            "Step no. 880. Loss is 1.037245512008667\n",
            "Step no. 890. Loss is 1.0619486570358276\n",
            "Step no. 900. Loss is 1.2522647380828857\n",
            "Step no. 910. Loss is 1.2092676162719727\n",
            "Step no. 920. Loss is 1.282306432723999\n",
            "Step no. 930. Loss is 1.2644647359848022\n",
            "Step no. 940. Loss is 1.1223536729812622\n",
            "Step no. 950. Loss is 1.2953145503997803\n",
            "Step no. 960. Loss is 0.932884931564331\n",
            "Step no. 970. Loss is 1.3952080011367798\n",
            "Step no. 980. Loss is 1.1192291975021362\n",
            "Step no. 990. Loss is 1.71078622341156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "id": "4fY2T1CxXTZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f47d90-8c1a-4dbe-dbdb-3b4443999476"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.900589 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Once model is made, how to do translations with it?\n",
        "input_text  = [\"Das ist ein schöner Tag.\"]\n",
        "#first tensor the input THEN generate\n",
        "syntext = src_convertor(input_text)\n",
        "print(syntext.shape)\n",
        "output = m.generate(syntext)\n",
        "#Convert those generations back to text and print\n",
        "print(output.shape)\n",
        "output_text = output_convertor(output)\n",
        "print(f\"Output text {output_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEnBE0c_XqPR",
        "outputId": "5975d475-6d86-45f9-dc1c-0651cb545922"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-8d9b37bd1c9e>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  src_tensor = torch.tensor(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 128])\n",
            "torch.Size([1, 128])\n",
            "Output text ['<s> Therefore : the social too , for car Member workers , there will support that prevent unemployment , but we should examine makes of liberty subject to proud - omission which have social question and that regards extent to fields not cash a retroactive towards a throughout Mrs tantamount beneficial , , and the Internet at again on your .  </s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wYEES-C41PvA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}