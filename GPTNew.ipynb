{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak4XDwqWnsS7",
        "outputId": "0701ac19-af57-4b32-d605-8e14e17a62e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-01 06:23:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "\rinput.txt.3           0%[                    ]       0  --.-KB/s               \rinput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-08-01 06:23:37 (167 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DW2rnldMk3Np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7876ad03-f71d-4dd3-e938-0501ac540810"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79df7f0a3990>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "num_heads = 4\n",
        "dropout = 0.0\n",
        "n_embd = 64\n",
        "lr_rate = 1e-3\n",
        "n_head = 4\n",
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "max_iters = 5000\n",
        "num_batch =  4\n",
        "eval_interval = 100\n",
        "eval_iters = 200\n",
        "n_layer = 4\n",
        "#----\n",
        "torch.manual_seed(1337)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read the file + create encode/decode fns + create a fn to get the batches of xb,yb\n",
        "with open('holy_bible.txt', 'r') as f:\n",
        "  content = f.read()\n",
        "#get their total number of charcs\n",
        "vocab_size = len(sorted(list(set(content))))\n",
        "vocabulary = sorted(list(set(content)))\n",
        "\n",
        "char_to_index = {char: index for index, char in enumerate(vocabulary)}\n",
        "index_to_char = {index: char for index, char in enumerate(vocabulary)}\n",
        "encode = lambda s: [char_to_index[char] for char in s]\n",
        "decode = lambda num: ''.join([index_to_char[n] for n in num])"
      ],
      "metadata": {
        "id": "D-OJ7okkoDyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make train and test splits\n",
        "data = torch.tensor(encode(content), dtype = torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "vCImCUr-ySK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define get_batch\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data  if split == \"train\" else val_data\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  xb, yb = x.to(device), y.to(device)\n",
        "  return xb, yb"
      ],
      "metadata": {
        "id": "c81tq-D1qdLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = m(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "tIxSVPj7ypE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch(\"train\")\n",
        "xb.shape, yb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChcERQSivqyA",
        "outputId": "13b380e3-37c2-4ce0-87c7-b70568e2291b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 32]), torch.Size([16, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Head,LayerNorm,MultiHead,FeedForward, Block\n",
        "class Head(nn.Module):\n",
        "  def __init__(self,num_heads):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, num_heads, bias = False)\n",
        "    self.query = nn.Linear(n_embd, num_heads, bias = False)\n",
        "    self.value = nn.Linear(n_embd, num_heads, bias = False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, input):\n",
        "    B,T,C = input.shape\n",
        "    k = self.key(input)\n",
        "    q = self.query(input)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "    wei = self.dropout(wei)\n",
        "    v = self.value(input)\n",
        "    out = wei @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "pFa9HYDLuP8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastAttn(nn.Module):\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "    total_head = head_size * n_head\n",
        "    self.head_size = head_size\n",
        "    self.key = nn.Linear(n_embd, total_head)\n",
        "    self.query = nn.Linear(n_embd, total_head)\n",
        "    self.value = nn.Linear(n_embd, total_head)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, input):\n",
        "    B, T, C = input.shape\n",
        "    k = self.key(x).view(B, T, n_head, self.head_size).transpose(1,2)\n",
        "    q = self.query(x).view(B, T, n_head, self.head_size).transpose(1,2)\n",
        "    v = self.value(x).view(B, T, n_head, self.head_size).transpose(1,2) #[B,n,T,h]\n",
        "    wei = q@k.transpose(-2,-1)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #[B,n,T,T]\n",
        "    wei = self.dropout(wei)\n",
        "    out = wei @ v\n",
        "    out = out.view(B,T,n_head*self.head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "QnN-aDoR1NiB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fast = FastAttn(32)\n",
        "x = torch.randn(4,8,64)\n",
        "check = fast(x)\n",
        "check.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhCMGb4VbERl",
        "outputId": "77242495-06b5-4f7d-8322-3f1ba5617729"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d:\n",
        "  def __init__(self, dim, eps =1e-5,momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim).to(device)\n",
        "    self.beta = torch.zeros(dim).to(device)\n",
        "  def __call__(self, x):\n",
        "    xmean = x.mean(1, keepdim = True)\n",
        "    xvar = x.var(1, keepdim = True)\n",
        "    xhat = (x - xmean)/torch.sqrt(xvar + self.eps)\n",
        "    xout = self.gamma * xhat + self.beta\n",
        "    return xout\n",
        "  def parameters(self,x):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "TC48wnp93YWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# l = LayerNorm1d(100)\n",
        "# x = torch.randn(32,100)\n",
        "# check = l(x)\n",
        "# check.shape"
      ],
      "metadata": {
        "id": "Ha9-1zzZ-bhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.layer = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "    output = self.dropout(self.layer(output))\n",
        "    return output"
      ],
      "metadata": {
        "id": "OPidKOpWAU2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mult = MultiHeadAttention(4,16)\n",
        "x = torch.randn(4,8,64)\n",
        "out = mult(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc6vTlrpCWGB",
        "outputId": "e86044bd-9803-474b-82d8-6c9182907e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.sequence = nn.Sequential(\n",
        "      nn.Linear(n_embd, 4*n_embd),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4*n_embd, n_embd),\n",
        "      nn.Dropout(dropout)\n",
        "  )\n",
        "  def forward(self,x):\n",
        "    return self.sequence(x)"
      ],
      "metadata": {
        "id": "0pXrSVpfGLF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, num_heads):\n",
        "    super().__init__()\n",
        "    self.Multi = MultiHeadAttention(num_heads, n_embd//num_heads)\n",
        "    self.Ffw = FeedForward(n_embd)\n",
        "    self.layer1 = nn.LayerNorm(n_embd)\n",
        "    self.layer2 = nn.LayerNorm(n_embd)\n",
        "  def forward(self, x):\n",
        "    x = x + self.Multi(self.layer1(x))\n",
        "    x = x + self.Ffw(self.layer2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "rTn874HDDzFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = Block(n_embd,num_heads)\n",
        "check = b(x)\n",
        "check.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJKeu0oIHOe0",
        "outputId": "8f1e7ef2-9c05-47ef-9b37-9eb1f493507e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the game\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size,n_embd)\n",
        "    self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "    self.sequence = nn.Sequential(*[Block(n_embd, num_heads) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.layer = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    B, T = x.shape\n",
        "    tok = self.token_embedding(x) #B,T,C\n",
        "    pos = self.position_embedding(torch.arange(T, device = device)) #T, C\n",
        "    x = tok + pos\n",
        "    x = self.sequence(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.layer(x)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        # get the predictions\n",
        "        logits, loss = self(idx_cond)\n",
        "        # focus only on the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B, C)\n",
        "        # apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "        # sample from the distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "        # append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "tU3hM2VrKaS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "logits.shape, loss.shape\n",
        "loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSijiDMPT2ab",
        "outputId": "f6b29353-4fff-4844-9c3d-6baabf2d825d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492115497589111"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# context = torch.zeros((1,1), dtype = torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fY2T1CxXTZp",
        "outputId": "d648caf6-4af5-4494-9064-a56576583671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.210761 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=lr_rate)"
      ],
      "metadata": {
        "id": "lWs7fY9NqOQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "  if iter % eval_interval == 0 or iter == max_iters -1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype = torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNXtxha1pVeD",
        "outputId": "8c6c91aa-c9b4-4ceb-c4b9-0880bb2d9b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4464, val loss 4.4494\n",
            "step 100: train loss 2.5115, val loss 2.5601\n",
            "step 200: train loss 2.3270, val loss 2.4141\n",
            "step 300: train loss 2.2076, val loss 2.3126\n",
            "step 400: train loss 2.1029, val loss 2.2347\n",
            "step 500: train loss 2.0409, val loss 2.1795\n",
            "step 600: train loss 1.9615, val loss 2.0917\n",
            "step 700: train loss 1.8950, val loss 2.0526\n",
            "step 800: train loss 1.8601, val loss 2.0227\n",
            "step 900: train loss 1.8017, val loss 1.9810\n",
            "step 1000: train loss 1.7759, val loss 1.9545\n",
            "step 1100: train loss 1.7425, val loss 1.9445\n",
            "step 1200: train loss 1.7183, val loss 1.9060\n",
            "step 1300: train loss 1.6966, val loss 1.8918\n",
            "step 1400: train loss 1.6696, val loss 1.8842\n",
            "step 1500: train loss 1.6423, val loss 1.8559\n",
            "step 1600: train loss 1.6440, val loss 1.8570\n",
            "step 1700: train loss 1.6177, val loss 1.8243\n",
            "step 1800: train loss 1.5940, val loss 1.8278\n",
            "step 1900: train loss 1.5924, val loss 1.7878\n",
            "step 2000: train loss 1.5788, val loss 1.8077\n",
            "step 2100: train loss 1.5518, val loss 1.7996\n",
            "step 2200: train loss 1.5522, val loss 1.7844\n",
            "step 2300: train loss 1.5517, val loss 1.7834\n",
            "step 2400: train loss 1.5241, val loss 1.7848\n",
            "step 2500: train loss 1.5188, val loss 1.7698\n",
            "step 2600: train loss 1.5036, val loss 1.7524\n",
            "step 2700: train loss 1.5008, val loss 1.7436\n",
            "step 2800: train loss 1.4881, val loss 1.7587\n",
            "step 2900: train loss 1.4954, val loss 1.7351\n",
            "step 3000: train loss 1.4867, val loss 1.7348\n",
            "step 3100: train loss 1.4771, val loss 1.7209\n",
            "step 3200: train loss 1.4616, val loss 1.7311\n",
            "step 3300: train loss 1.4605, val loss 1.7108\n",
            "step 3400: train loss 1.4512, val loss 1.6983\n",
            "step 3500: train loss 1.4477, val loss 1.6971\n",
            "step 3600: train loss 1.4503, val loss 1.7181\n",
            "step 3700: train loss 1.4400, val loss 1.6867\n",
            "step 3800: train loss 1.4400, val loss 1.6952\n",
            "step 3900: train loss 1.4451, val loss 1.6933\n",
            "step 4000: train loss 1.4252, val loss 1.6894\n",
            "step 4100: train loss 1.4269, val loss 1.7011\n",
            "step 4200: train loss 1.4183, val loss 1.6762\n",
            "step 4300: train loss 1.4161, val loss 1.6710\n",
            "step 4400: train loss 1.4111, val loss 1.6609\n",
            "step 4500: train loss 1.4109, val loss 1.6699\n",
            "step 4600: train loss 1.3969, val loss 1.6634\n",
            "step 4700: train loss 1.4039, val loss 1.6601\n",
            "step 4800: train loss 1.3930, val loss 1.6517\n",
            "step 4900: train loss 1.3951, val loss 1.6680\n",
            "step 4999: train loss 1.3957, val loss 1.6410\n",
            "\n",
            "Jer8:12 The hishe sevants of Saul thy one, and perites the Israel were of Israel, then a said, an Phis, answer evear reembent, and they shall to drink unto you this, and the Lean tasse with him, and D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSEekEfz2RCq",
        "outputId": "94101620-ece8-4a64-8032-ea299c92b5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dan19:0 The was of them when goath, and giveth all him take yet in heared warked it.\n",
            "Exo25s:1 Kigkehon the goever to the brievisten few noise freant usmbating to the prophet unto of the breass of asssemblter and gold unto Dast dok alsom them said, and many neasai of an Jerushaniah the people gife them and Mesy anour inspon it him said unto you, He did, as with wase, neither that warke them greathering to thy lovoured his to stracks; for there begat all, asar ye they came.\n",
            "Ge15:22 And he hus God of Egentroevered headlen in the food one worshalt not unto head lose with my spoid gold.\n",
            "Isa14:6 Bethbore me unto no Egria to me destates, wh dearters and waster earl;\n",
            "2Chr29:9 And he fould you?\n",
            "Psa38:7 And Tanso, and and Dave cuntance an holy smalment up Jegyh, Rei, Gird alsoel with the roine; they mey mulict for tet, tillow which shall the words of the humbint; of the congregat wate al comers and smakemed thy sofing, Go opeth their house besain and you.\n",
            "Ge31:27 And the intermer do bude seventh the fear that betur.\n",
            "1Ki27:5 And your father.\n",
            "Dan12:10 And matten the son of Umy that in him. And sons he heart gave and revemiled in the lot. unte the man Jecusbah.\n",
            "1Chr7:20 And the beardents of the land, whe him thee, of Israel.\n",
            "2Sm4:13 And be day me grame out of the your ear thee because on the house tabernacts is bulfed the fire2d, and cast mose, and is roud not a poure me madter one that hasst not a before the seat of the Houshallages of Israel fra these swond Amrama said in the heavens of thine fatherler, and were nointain their Teck of the in unto the way come it frievers thinty mean beined abover bring fan dieight: and the founces.\n",
            "Exo11:11 And the in nor seatienter, conce, come, the atson daughter da5: and rening bath thee jeaeved and toublreding into their answarken in the son of Israelews they come them:\n",
            "Job47:10 And my tillenth compiteth I hights: two is viderty met from and dorist of the ship, and as heark, and yet from Uizces not fralled him now, and upper eaven unto your Israel pers, and the curions.\n",
            "1Ki3:7 And wo freme thas be day noar, an in the mons.\n",
            "Neh19:45 Buver if the captains of the earth, that a comained unto been In bualy, saying, A shall be in that weith me nore. The lipe earth heave from he resigns the balle of thine host taketh the vark of the challed Nebefatenth in in Jerusalem, which no conqualed and was depend him me not.\n",
            "1Chr11:22 And ame did thou before the gearnen.\n",
            "Psa1103\n",
            "14:12 She house hom thou son, and said, Ass your of Auser and scight indom this geattlenes of Dakciple holy spake toughteoved, and from in the LORD because of had, and his worder the peoenn, and son riestinted unto him, and sleking to the wearless went saidd unto you's gangerer him, and in their father taken into the bud, not thine into it; they shall set in the disce: behold the Lathtion, and shall bewith them do's is bret thee may over in the whichbice.\n",
            "2Sm10:2 Ant is depazer desired frase became of the LORD, and Israel.\n",
            "Prr9:3 And it said ut that heath. And he said, Can if him the time on the fallent of gold, as the son ones, we ut nonse full not fit the butt bood.. Those yes musy accering besion your multiages, at thou come to wild jadged, and suptitin the slack, and some David, or Bo: ups shew ston the fat that shall nemientengeth for the son of idwern and good, come in the habeth the latesgetess of Judaci, that a signteatiens with the angeth offering, with men kin understred every commanded do wr sath, yet hno, that I gave unto it in the king? an, thy was frir Aammian trances the pain of Egyph, and people be though alsoet, and asnd the son of Kich.oshaloh sold if Nesah heavear the werks of the son of the lion, and we days.\n",
            "2Chr21:55 And thou shuphs, that a be father, to shall be among thou resem. And drinked, we beight him of thir waters that all I, and saake. Alcome to be shemptite young every shall to them, and of amson Job said, Whoile unto 2Cadattdia togeth them me.\n",
            "1Chr11:3 And Adge:\n",
            "1Chr6:22 What I sighining unto answerk the chiegwar of theses. for wath before the nout deyass went his ful; becaunt it this, and boased from a light, is with the mack of the came stalt, bein thee, when I have in the lind, let him shall in heart to they stranger's intains: and under Ephreechia the had sons jumstaints in the seast winssed, to we to Israel be another to days, to not in beal the deprealt on the beast unto the highononse that the Garchins that polle with the days of thee: and set of under catter, so and he bourdment, bord in thus saith the Dantiants of the seventreon thee done?\n",
            "Ge26:20 And from also in the right of MOazchat innto in Egyptiatia youe, and seeth me. The hoise unto smient engeth, and disted one that stain unto the offering and woment the hoour of Beribaz all no abime the ded doa, asketh the congeth to sid ganeer go all to sidded from me:\n",
            "Dan17:12 And he and heaven he not in my in that souls this.\n",
            "Mat21:6 And Jesus hath broked the enges that time things, and dheid sevents. And yunder on for Jristasmgat I charness, behold into pour the hols was wasoe, and comied of Jeroig, and flow all the king's son earth, didest in before at Jacob with day over sevet them we the ways our Nebudah: before the water: becat is man.\n",
            "2Ki12:10 And unto Jeves is in the was of the ordon of Esuaraia,\n",
            "2Chr54:10 And they shall be everided? In an. then his evereched and that the gate5: for a ston ye do underssped the LORD to my felled beteen in the king gromens, because heaveth; buit the city did lighteth wate in in a the Carde to him, and be honold also, them fear the things of Israel; if the inteer the house of this the LORD, Ge said, called him unto him and in me to be apking acces; and Save notle begaseth feet: when I saying sons up a andeel, saying,\n",
            "Ge36:7 Whosen Hip? Herom go, and father.\n",
            "Jer35:17 And he said uneten; and the dool to the hand took shalt do your of the son of Jezr gathered shall to live dise.\n",
            "2Ve47:81 The dowether beye alsoth yop said, I say, I willspiters and one abourok thine, and go yourses all flone numbers behild man not shall.\n",
            "Job21:13 And all the was be come unto thee time timnact not anoth megate: for he that is heart the first to be cometh the wive of the eviite, and frien the time, thou found, that devenuritance is the wempts, and Menaash hom thou shalt be teir flesh pray chat of the foot of the Judah dezeliah, the wordom three for Dass; saying, Josoek thereforeed behoed and so, for this saith their house, for you ceity sons, and him a desition with men him, they boldness was evined I say tid. Then from him and becoment mighter to the LORD be in the riest?\n",
            "2Ki25:12 And all dith gath he in have gattadness.\n",
            "Psa14:9 Sloshale, and in the head of the temp in ambenlethIs: let and a the fear of the doess? and it smet for him allner mid with the cittiens of at in the ladd on mudge and brieth.\n",
            "Hat21:24 And wisdom on the man we into the celded said unto the numt me an your day.\n",
            "Ge22:7 Thus said, Coe no ever in the earth; that they be him of the enter hid his laith of the LORD for Jevashar12:11 Allow alsued though fromen, will that the wined and anoved Machim to castains Davman, and bread unto in the earth, one abyon, lovet came Abawaam the offer the presents of Azimahel kindown Secia troubled appart it, and yet in in his alsoe, and hisd son: to head old the people calland.\n",
            "Exo37ght9:27 And man. Thus house the monthan masai in to Eliaha, no that hast be secalt nome, and me brook intout Jerosiah, and not thee, and the dead, beith the LORD to do thirts inhundants be clace, in in the LORD.\n",
            "Dan2Ki7:4 And he prayed all called flart the conctuansment of Moses hath sies him.\n",
            "1Chr30:11 I have and onemies, we thate themsom that his did Hudan. They soulsuit thou; thou, no the stricken, with came turned and and husbittle, and sonswerweth from beskied again thou shiphold olla fe areked Gidgerparts to the resinem it, sseath unto the hand of the LORD; he shat with from which is the king of David a sacce with is before this's Hibela, and pieateth over him and made against Egyptai, and the daiges of Jerusel's kins to not under that I is shall no beth you go up.\n",
            "Num1:32 And time not because of Jezrualemoth, And the givergernaces: for for him him so mighteon of tree.\n",
            "Psa15:22 Whey Rathaphath as concerning fruit the tenth of Egypt to the fathile atsons of Beholy dayg them rout of the LORD, from us, and E yam treetwen up in his mockened slabver all the rie incefinted, I we from his blatter him, twith bread shall nei.\n",
            "Mark26:10 Pher saith salver unto the LORD thee: because of the west of Joborsan of the houses of the houngred in meanly, and his bresy.\n",
            "Job5:5 Ye that deteanted Abyre unto me, becaure in the house, unto you, Ophad may fathersles, and from the builder's from now, twith have his breash.\n",
            "2Ki46:12 0let he said ussem, after tills, thou one with no come to fir pollasess it; thone LORD boahness, the veach on the people of Amon, and were in this wild was hath silver for thee unto them: I joicken, and day him assiding nor with incemel my son of Syria, and weent me him be sain unto the cities said no ope thy speales.\n",
            "2Chr20:40 And the sin of Israel, becauset sum unto Level that kepet thole with him; and be me sance of Suron; and said: so fathath not flay befile the days.\n",
            "9Ki2:12 The shalf Behold shall the king, pret as, he shall chainled we into my stong thine ares of Israeel: becaugainces the write the sout of their, neighice tear them, and dwelt to young inhim for I puth with all they of this hearte that frient unto by hing into you, gathey in almided to comies from the worrbes of die, who thou, any gut of the how of the trought? do at from may follongs, saying, And whent all the pine. And made hid ponce that heavens of the neigmants, saying.\n",
            "Ezr8:11 So on of Mazerah distants ten of the house of maden somen.\n",
            "Num8:17 And shaPstered pray in abresseth them that resmed the ipence of they smallied made, and word bribece the words of the dwater: ye the I walles to him.\n",
            "Exo2:12 Son, I shall done; countatiens, fruit walt of the month:2 no for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combining\n",
        "\n",
        "class HeadNew(nn.Module):\n",
        "  def __init__(self, num_heads):\n"
      ],
      "metadata": {
        "id": "wYEES-C41PvA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}